{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_-sVEIDhd-J"
   },
   "source": [
    "# Learning a Restricted Boltzmann Machine with sampling\n",
    "In addition to combinatorial optimization problems, QUBO can also be used to compute a Boltzmann machine, which use probability distribution models of problem including global or local optimum.  \n",
    "Here we review the sampling method in Boltzmann machine with a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JUM_rWQs459"
   },
   "source": [
    "## What is RBM (Restricted Boltzmann Machine)?\n",
    "RBM (Restricted Boltzmann Machine) is a model with a restricted network structure for a stochastic network model called a Boltzmann machine.\n",
    "\n",
    "reference:\n",
    "Restricted Boltzmann machine\n",
    "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xK84qUBtd0o"
   },
   "source": [
    "## Reference Materials\n",
    "It all started with this paper from the University of Montreal.  \n",
    "A well-known resercher in deep learning presented the following paper as a possible implementation of RBM in D-Wave, and its implementation on D-Wave machines was considered.\n",
    "\n",
    "On the Challenges of Physical Implementations of RBMs\n",
    "Vincent Dumoulin and Ian J. Goodfellow and Aaron Courville and Yoshua Bengio\n",
    "https://arxiv.org/pdf/1312.5258.pdf\n",
    "\n",
    "A policy was explored that sampling could be used to estimate the gradient of the RBM NLL(negative logarithmic likelihoods) needed for training using a software simulator that mimics D-Wave, but the actual machine was not used.  \n",
    "This paper and the following article by Dr. Geoffrey Hinton are very helpful in learning about RBM.\n",
    "\n",
    "A Practical Guide to Training Restricted Boltzmann Machines\n",
    "Geoffrey Hinton\n",
    "https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gJPAt8kth_O"
   },
   "source": [
    "## Advocacy of Boltzmann sampling\n",
    "Next, as a result of actually running the above problems on a D-Wave machine,\n",
    "\n",
    "Application of Quantum Annealing to Training of Deep Neural Networks\n",
    "Steven H. Adachi, Maxwell P. Henderson\n",
    "https://arxiv.org/abs/1510.06356\n",
    "\n",
    "This takes a specific policy of using the quantum annealer as a Boltzmann sampling machine to help in the estimation of the NLL described above, and the training is actually being done on an actual machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBvepLKCttWW"
   },
   "source": [
    "## RBM Model Overview\n",
    "First, the model consists of two layers, called the visible layer and the hidden layer. It is an undirected graph with no directional coupling.\n",
    "\n",
    "<img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_0.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSAV3mkUC6Qq"
   },
   "source": [
    "The probability distribution follows a Boltzmann distribution as shown below.\n",
    "\n",
    "$$p(v,h) = \\frac{1}{Z}exp(-E(v,h))$$\n",
    "\n",
    "This probability distribution is specified by the energy function and is shown below for the number of nodes in the visible layer n and the number of nodes in the hidden layer m.\n",
    "\n",
    "$$E(v,h) = -\\sum_{i=1}^n b_i v_i - \\sum_{j=1}^m c_j h_j -\\sum_{i=1}^n\\sum_{j=1}^m W_{ij}v_ih_j$$\n",
    "\n",
    "The normalization constants (distribution functions) is as follows.\n",
    "\n",
    "$$Z = \\sum_{v_k}\\sum_{h_l}exp\\left( \\sum_kb_kv_k + \\sum_l c_l h_l + \\sum_{k,l} W_{kl}v_k h_l \\right)$$\n",
    "\n",
    "From the complete bipartite graph, the conditional probability distributions are as follows using the sigmoid function for $v$ and $h$, respectively.\n",
    "\n",
    "$$p(h_j = 1|v) = sigm(c_j+\\sum_iW_{ij}v_i) \\\\p(v_i = 1|h) = sigm(b_i+\\sum_jW_{ij}h_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXD5eLtYwObE"
   },
   "source": [
    "## About Learning\n",
    "Next, we want to see how to train the RBM model consisting of the above probability distribution.  \n",
    "Even with DBM (Deep Boltzmann Machine), having multi-layers, the learning is done in the form of RBM.  \n",
    "These trainings perform error calculations on the training data and the model so as to maximize the log-likelihood $\\log{P}$.  \n",
    "The gradient calculation of the coupling coefficients and bias is shown below using $\\log{P}$.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_1.png?raw=1\" width=800>\n",
    "</div>\n",
    "\n",
    "Here, there is not a very efficient calculation of the expected value of the model in the gradient calculation of the coupling coefficient above.\n",
    "\n",
    "<div>\n",
    "<img class=\"math math-inline\" src=\"https://render.githubusercontent.com/render/math?math=%3D%20%5Cfrac%7B1%7D%7BZ%7D%5Csum_%7B%5C%7Bv_k%5C%7D%7D%5Csum_%7B%5C%7Bh_l%5C%7D%7Dv_ih_j%20exp%5Cleft(%5Csum_kb_kv_k%20%2B%20%5Csum_lc_lh_l%2B%5Csum_%7Bkl%7DW_%7Bkl%7Dv_kh_l%5Cright)%0D%0A\" width=500>\n",
    "</div>\n",
    "\n",
    "In practice, it is very difficult to obtain this value directly, so the Gibbs sampling method, such as the CD method, is used to calculate the hidden layer and the visible layer in order to obtain the value.  \n",
    "The CD method is fairly approximate to save computational cost, and improving the accuracy of this method is computationally expensive and time-consuming.\n",
    "\n",
    "The above is an update of the coupling factor, but the same is true for the bias update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRWk0Z7YxWyD"
   },
   "source": [
    "## Parameter updates using Boltzmann sampling\n",
    "Here, instead of this CD method, we're going to use an annealing machine to calculate the gradient, which is the basis of the Boltzmann sampling study using the actual machine.\n",
    "\n",
    "The coupling coefficients and bias update equations when using Boltzmann sampling are as follows.  \n",
    "$\\alpha$ is momentum and $\\epsilon$ is learning rate.\n",
    "\n",
    "<dev>\n",
    "    <img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_3.png?raw=1\" width=700>\n",
    "</dev>\n",
    "\n",
    "After learning the RBM, back-propagation study with a classical calculator is done to finish it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKabmUacx6ud"
   },
   "source": [
    "## Sampling methods using annealing\n",
    "\n",
    "The D-Wave machine is built on the theory of quantum annealing, which is basically designed to find a minimum value to solve an optimization problem.\n",
    "In practice, however, the effects of the external environment and other reasons often prevent us from settling on an optimal solution.\n",
    "Therefore, the basis of sampling learning is to use this property of falling into a local solution as a sampling machine to find a distribution.\n",
    "\n",
    "If we can assume that the variability of the excited states is a Boltzmann distribution, we can approximate it to the following equation.\n",
    "The correspondence between the following equation and Boltzmann distribution above allow us to introduce the sampling method into the update equation.\n",
    "\n",
    "<dev>\n",
    "    <img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_4.png?raw=1\" width=\"250\">\n",
    "</dev>\n",
    "\n",
    "$H_f$ is the cost function to be found final, and $\\beta_{eff}$ is the variable that adjusts the distribution of the sampling.  \n",
    "By using it, we can approximate the most computationally intensive part as follows\n",
    "\n",
    "<dev>\n",
    "    <img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_5.png?raw=1\" width=500>\n",
    "</dev>\n",
    "\n",
    "We apply this to the model expectation values.  \n",
    "The same applies to model expectations for other visible and hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cz7rdeWMy857"
   },
   "source": [
    "## Run with a simple example.\n",
    "Let's learn about the above sampling for RBM in a simple example.  \n",
    "We'll start with a simple question, which is also listed in the D-Wave examples.\n",
    "\n",
    "<dev>\n",
    "    <img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_6.png?raw=1\" width=150>\n",
    "</dev>\n",
    "\n",
    "In the case of this QUBOmatrix, the cost function is\n",
    "\n",
    "<dev>\n",
    "<img class=\"math math-inline\" src=\"https://render.githubusercontent.com/render/math?math=E(x)%20%3D%20-x_1-x_2%2B2x_1x_2\" width=200>\n",
    "<\\dev>\n",
    "\n",
    "If we look for the energy in the number of cases of x, we see that\n",
    "\n",
    "<dev>\n",
    "    <img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_7.png?raw=1\" width=150>\n",
    "</dev>\n",
    "\n",
    "Thinking analytically, we can get the numbers for all cases here, so we first find the normalized constant Z.\n",
    "\n",
    "<dev>\n",
    "    <img class=\"math math-inline\" src=\"https://render.githubusercontent.com/render/math?math=Z%20%3D%20%5Csum%20exp(E(x))%20%3D%20exp(0)%20%2B%20exp(1)%20%2B%20exp(1)%20%2B%20exp(0)%20%3D%201%2B2.718%2B2.718%2B1%20%3D%207.44\" width=700>\n",
    "</dev>\n",
    "\n",
    "and\n",
    "\n",
    "<dev>\n",
    "    <img class=\"math math-inline\" src=\"https://render.githubusercontent.com/render/math?math=%7BP%20%3D%20%5Cfrac%7B1%7D%7BZ%7Dexp(E)%0D%0A%7D\" width=120>\n",
    "</dev>\n",
    "\n",
    "so probability is calculated as 0.13 and 0.37.\n",
    "\n",
    "<dev>\n",
    "    <img src=\"https://github.com/Blueqat/Blueqat-tutorials/blob/master/tutorial-ja/img/021_8.png?raw=1\" width=200>\n",
    "</dev>\n",
    "\n",
    "It's hard to get this out by hand, so I'll try to think of it as a sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8QYrCcR1Ihs"
   },
   "source": [
    "## Sampling with Wildqat\n",
    "Sampling is easy. Run the above QUBO multiple times and take a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "BYv3Xy4meYwC",
    "outputId": "7929d412-6146-412a-a50f-69559e57b2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blueqat\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/86/1b72a7cbe500b861d63e84cc6383fbf3730f08ae69fcd85146ae8e3b8873/blueqat-0.3.10-py3-none-any.whl (46kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy~=1.12 in /usr/local/lib/python3.6/dist-packages (from blueqat) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from blueqat) (1.3.3)\n",
      "Installing collected packages: blueqat\n",
      "Successfully installed blueqat-0.3.10\n"
     ]
    }
   ],
   "source": [
    "!pip install -U blueqat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn5qPBidedWy"
   },
   "outputs": [],
   "source": [
    "import blueqat.wq as wq\n",
    "import numpy as np\n",
    "a = wq.Opt()\n",
    "\n",
    "beta = 0.05\n",
    "a.R = 0.5\n",
    "\n",
    "a.qubo = np.asarray([[-1,2],[0,-1]])*beta\n",
    "qarr = [[0,0],[0,1],[1,0],[1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OLJnB7tc6yNh",
    "outputId": "0e4506f3-6606-4d6a-cd40-4fc446a895e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 385, 406, 105]\n"
     ]
    }
   ],
   "source": [
    "cnt = [0]*4\n",
    "\n",
    "for i in range(1000):\n",
    "    b = a.sa()\n",
    "  \n",
    "    for j in range(4):\n",
    "        if b == qarr[j]:\n",
    "            cnt[j] += 1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFS5qGq28YK6"
   },
   "source": [
    "The above shows the probability of occurrence of qarr = [[0,0],[0,1],[1,0],[1,1]], with the lower energy being more likely to appear.  \n",
    "\n",
    "Since the total number of trials was 1,000 this time, we can find the probability distribution by multiplying 1/1000 to each number of appearance.\n",
    "This is put into the update formula to update the coupling coefficineces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "044_rbm_sampling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
